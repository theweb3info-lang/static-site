# 200 å…¬é‡Œå…‰çº¤å½“å†…å­˜ï¼Ÿâ€”â€” John Carmack çš„ç–¯ç‹‚å®éªŒèƒŒåçš„æ·±åˆ»æ´å¯Ÿ

## ğŸ“Œ å­¦ä¹ ç›®æ ‡

After reading this article, you will understand:
- Why latency is the bottleneck in modern computing
- How fiber optic cable could theoretically work as memory
- What Carmack's proposal reveals about AI architecture
- Why traditional memory hierarchy might be obsolete

è¯»å®Œæœ¬æ–‡åï¼Œä½ å°†ç†è§£ï¼š
- ä¸ºä»€ä¹ˆå»¶è¿Ÿæ˜¯ç°ä»£è®¡ç®—çš„ç“¶é¢ˆ
- å…‰çº¤å¦‚ä½•åœ¨ç†è®ºä¸Šå……å½“å†…å­˜
- Carmack çš„ææ¡ˆæ­ç¤ºäº†ä»€ä¹ˆå…³äº AI æ¶æ„çš„æ´å¯Ÿ
- ä¸ºä»€ä¹ˆä¼ ç»Ÿå†…å­˜å±‚æ¬¡å¯èƒ½å·²ç»è¿‡æ—¶

---

## ğŸ¬ Opening: A Tweet That Broke the Internet

> "200 kilometers of fiber would have the same latency as DRAM."
> 
> â€” John Carmack, 2026

> "200 å…¬é‡Œçš„å…‰çº¤ï¼Œå»¶è¿Ÿå’Œ DRAM ç›¸å½“ã€‚"
> 
> â€” John Carmack, 2026

When the legendary game developer John Carmack (creator of *Doom* and *Quake*) tweeted this, the internet exploded. Some laughed. Some were confused. A few understood what he was really saying.

å½“ä¼ å¥‡æ¸¸æˆå¼€å‘è€… John Carmackï¼ˆã€Šæ¯ç­æˆ˜å£«ã€‹å’Œã€Šé›·ç¥ä¹‹é”¤ã€‹çš„åˆ›é€ è€…ï¼‰å‘å‡ºè¿™æ¡æ¨æ–‡æ—¶ï¼Œäº’è”ç½‘ç‚¸äº†ã€‚æœ‰äººå˜²ç¬‘ï¼Œæœ‰äººå›°æƒ‘ï¼Œå°‘æ•°äººç†è§£äº†ä»–çœŸæ­£æƒ³è¯´çš„ã€‚

Let me ask you a question: **If I told you that a fiber optic cable stretching from Beijing to Tianjin could replace your computer's RAM, would you believe me?**

è®©æˆ‘é—®ä½ ä¸€ä¸ªé—®é¢˜ï¼š**å¦‚æœæˆ‘å‘Šè¯‰ä½ ï¼Œä¸€æ ¹ä»åŒ—äº¬åˆ°å¤©æ´¥çš„å…‰çº¤å¯ä»¥ä»£æ›¿ä½ ç”µè„‘çš„å†…å­˜ï¼Œä½ ä¼šç›¸ä¿¡å—ï¼Ÿ**

Sounds insane, right? But before dismissing it, let's understand what Carmack is actually proposing â€” and why it's more brilliant than crazy.

å¬èµ·æ¥å¾ˆç–¯ç‹‚ï¼Œå¯¹å§ï¼Ÿä½†åœ¨å¦å®šå®ƒä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆç†è§£ Carmack å®é™…åœ¨æè®®ä»€ä¹ˆ â€”â€” ä»¥åŠä¸ºä»€ä¹ˆè¿™æ¯”ç–¯ç‹‚æ›´åŠ ç¿æ™ºã€‚

---

## Part 1: Understanding Latency â€” The Real Enemy

### What is latency?

Think of latency as **waiting time**. When your brain tells your hand to move, there's a tiny delay before your hand actually moves. That delay is latency.

æŠŠå»¶è¿Ÿæƒ³è±¡æˆ**ç­‰å¾…æ—¶é—´**ã€‚å½“ä½ çš„å¤§è„‘å‘Šè¯‰ä½ çš„æ‰‹ç§»åŠ¨æ—¶ï¼Œä½ çš„æ‰‹å®é™…ç§»åŠ¨ä¹‹å‰æœ‰ä¸€ä¸ªå¾®å°çš„å»¶è¿Ÿã€‚é‚£ä¸ªå»¶è¿Ÿå°±æ˜¯å»¶è¿Ÿã€‚

In computers, latency is the time between:
1. CPU asking for data: "Give me the value at address 0x1000"
2. Memory responding: "Here it is: 42"

åœ¨è®¡ç®—æœºä¸­ï¼Œå»¶è¿Ÿæ˜¯ä»¥ä¸‹ä¸¤è€…ä¹‹é—´çš„æ—¶é—´ï¼š
1. CPU è¯·æ±‚æ•°æ®ï¼š"ç»™æˆ‘åœ°å€ 0x1000 çš„å€¼"
2. å†…å­˜å“åº”ï¼š"åœ¨è¿™é‡Œï¼š42"

### The Latency Ladder

Here's how different storage types compare:

ä»¥ä¸‹æ˜¯ä¸åŒå­˜å‚¨ç±»å‹çš„æ¯”è¾ƒï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           The Latency Ladder                    â”‚
â”‚           (å»¶è¿Ÿé˜¶æ¢¯)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CPU Register    â”‚ ~0.3ns  â”‚ â–“                 â”‚
â”‚  L1 Cache        â”‚ ~1ns    â”‚ â–“â–“                â”‚
â”‚  L2 Cache        â”‚ ~10ns   â”‚ â–“â–“â–“â–“â–“             â”‚
â”‚  L3 Cache        â”‚ ~40ns   â”‚ â–“â–“â–“â–“â–“â–“â–“â–“          â”‚
â”‚  DRAM (RAM)      â”‚ ~100ns  â”‚ â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“   â”‚
â”‚  SSD             â”‚ ~100Î¼s  â”‚ â–“â–“â–“â–“â–“â–“â–“â–“... (100x) â”‚
â”‚  HDD             â”‚ ~10ms   â”‚ â–“â–“â–“â–“â–“â–“... (100,000x)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Notice the pattern: **Faster = More expensive = Less capacity**

æ³¨æ„è¿™ä¸ªè§„å¾‹ï¼š**æ›´å¿« = æ›´è´µ = å®¹é‡æ›´å°**

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ å¦‚æœè®©ä½ å‘å°æœ‹å‹è§£é‡Šâ€¦â€¦
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Imagine your desk:
- **Register**: Paper in your hand (instant access)
- **L1 Cache**: Papers on your desk (1 second to grab)
- **RAM**: Files in your drawer (10 seconds to open)
- **SSD**: Files in your cabinet (1 minute to fetch)
- **HDD**: Files in your basement (10 minutes to retrieve)

æƒ³è±¡ä½ çš„ä¹¦æ¡Œï¼š
- **å¯„å­˜å™¨**ï¼šä½ æ‰‹é‡Œçš„çº¸ï¼ˆç¬é—´è®¿é—®ï¼‰
- **L1 ç¼“å­˜**ï¼šä½ æ¡Œä¸Šçš„çº¸ï¼ˆ1 ç§’æ‹¿åˆ°ï¼‰
- **RAM**ï¼šä½ æŠ½å±‰é‡Œçš„æ–‡ä»¶ï¼ˆ10 ç§’æ‰“å¼€ï¼‰
- **SSD**ï¼šä½ æŸœå­é‡Œçš„æ–‡ä»¶ï¼ˆ1 åˆ†é’Ÿå–å‡ºï¼‰
- **HDD**ï¼šä½ åœ°ä¸‹å®¤çš„æ–‡ä»¶ï¼ˆ10 åˆ†é’Ÿæ‰¾åˆ°ï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

### Why does latency matter?

Because **the CPU is always waiting**. Modern CPUs can execute billions of instructions per second, but if they have to wait 100ns for data from RAM, they spend most of their time doing... nothing.

å› ä¸º **CPU æ€»æ˜¯åœ¨ç­‰å¾…**ã€‚ç°ä»£ CPU æ¯ç§’å¯ä»¥æ‰§è¡Œæ•°åäº¿æ¡æŒ‡ä»¤ï¼Œä½†å¦‚æœå®ƒä»¬å¿…é¡»ç­‰å¾… 100ns æ‰èƒ½ä» RAM è·å–æ•°æ®ï¼Œå®ƒä»¬å¤§éƒ¨åˆ†æ—¶é—´éƒ½åœ¨åšâ€¦â€¦ä»€ä¹ˆéƒ½ä¸åšã€‚

This is called the **memory wall** â€” and it's getting worse every year.

è¿™è¢«ç§°ä¸º**å†…å­˜å¢™** â€”â€” è€Œä¸”æ¯å¹´éƒ½åœ¨æ¶åŒ–ã€‚

---

## Part 2: The Traditional Memory Hierarchy

### How computers manage memory today

Modern computers use a **pyramid of caches** to hide latency:

ç°ä»£è®¡ç®—æœºä½¿ç”¨**ç¼“å­˜é‡‘å­—å¡”**æ¥éšè—å»¶è¿Ÿï¼š

```
        â”Œâ”€â”€â”€â”€â”€â”
        â”‚ CPU â”‚  â† The brain (å¤§è„‘)
        â””â”€â”€â”¬â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ L1 Cache    â”‚  â† Smallest, fastest (æœ€å°æœ€å¿«)
    â”‚   ~32 KB    â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ L2 Cache    â”‚  â† Bigger, slower
    â”‚  ~256 KB    â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ L3 Cache    â”‚  â† Even bigger, even slower
    â”‚   ~8 MB     â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚    DRAM     â”‚  â† Main memory (ä¸»å†…å­˜)
    â”‚   16 GB     â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚     SSD     â”‚  â† Storage (å­˜å‚¨)
    â”‚   1 TB      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The problem: Power consumption

Here's something that might surprise you: **DRAM is hungry**.

è¿™å¯èƒ½ä¼šè®©ä½ æƒŠè®¶ï¼š**DRAM å¾ˆè€—ç”µ**ã€‚

A modern data center spends:
- **40% of power** on DRAM (keeping it refreshed)
- **30% of power** on computation
- **30% of power** on cooling

ä¸€ä¸ªç°ä»£æ•°æ®ä¸­å¿ƒçš„åŠŸè€—åˆ†å¸ƒï¼š
- **40% çš„åŠŸè€—**ç”¨äº DRAMï¼ˆä¿æŒåˆ·æ–°ï¼‰
- **30% çš„åŠŸè€—**ç”¨äºè®¡ç®—
- **30% çš„åŠŸè€—**ç”¨äºå†·å´

Why? Because DRAM is **dynamic** â€” it forgets data every few milliseconds unless refreshed. It's like trying to hold water in your hands: you have to constantly cup them or it leaks out.

ä¸ºä»€ä¹ˆï¼Ÿå› ä¸º DRAM æ˜¯**åŠ¨æ€çš„** â€”â€” å®ƒæ¯éš”å‡ æ¯«ç§’å°±ä¼šå¿˜è®°æ•°æ®ï¼Œé™¤éåˆ·æ–°ã€‚å°±åƒè¯•å›¾ç”¨æ‰‹æ§æ°´ï¼šä½ å¿…é¡»ä¸æ–­åœ°åˆæ‹¢åŒæ‰‹ï¼Œå¦åˆ™æ°´ä¼šæ¼æ‰ã€‚

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ å¸¸è§è¯¯åŒº
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âŒ **è¯¯åŒºï¼š** "RAM is just storage that happens to be fast."
âŒ **è¯¯åŒºï¼š** "RAM åªæ˜¯æ°å¥½å¾ˆå¿«çš„å­˜å‚¨å™¨ã€‚"

âœ… **çœŸç›¸ï¼š** RAM is a **power-hungry, constantly-refreshing, expensive** component that only exists because CPUs need random access.
âœ… **çœŸç›¸ï¼š** RAM æ˜¯ä¸€ä¸ª**è€—ç”µã€ä¸æ–­åˆ·æ–°ã€æ˜‚è´µ**çš„ç»„ä»¶ï¼Œå®ƒå­˜åœ¨çš„å”¯ä¸€åŸå› æ˜¯ CPU éœ€è¦éšæœºè®¿é—®ã€‚
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

---

## Part 3: Carmack's Proposal â€” Fiber as Cache

### The crazy idea

John Carmack proposed: **What if we use a loop of fiber optic cable as a cache?**

John Carmack æè®®ï¼š**å¦‚æœæˆ‘ä»¬ç”¨ä¸€åœˆå…‰çº¤ä½œä¸ºç¼“å­˜ä¼šæ€æ ·ï¼Ÿ**

Here's how it works:

ä»¥ä¸‹æ˜¯å®ƒçš„å·¥ä½œåŸç†ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Fiber Optic Delay Line Memory          â”‚
â”‚      (å…‰çº¤å»¶è¿Ÿçº¿å­˜å‚¨å™¨)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚Laser  â”‚ â”€â”€â†’  [Fiber Loop]  â”€â”€â†’â”‚ Photo â”‚  â”‚
â”‚   â”‚Sender â”‚       200 km          â”‚ Det.  â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚       â–²                              â”‚      â”‚
â”‚       â”‚                              â”‚      â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€ Data Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                              â”‚
â”‚   Light travels at ~200,000 km/s             â”‚
â”‚   200 km Ã· 200,000 km/s = ~1 ms             â”‚
â”‚   å…‰é€Ÿçº¦ 200,000 å…¬é‡Œ/ç§’                     â”‚
â”‚   200 å…¬é‡Œ Ã· 200,000 å…¬é‡Œ/ç§’ = ~1 æ¯«ç§’      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Wait, 1ms = 100ns?

You might notice: **1 millisecond â‰  100 nanoseconds**. In fact, 1ms = 1,000,000ns!

ä½ å¯èƒ½æ³¨æ„åˆ°äº†ï¼š**1 æ¯«ç§’ â‰  100 çº³ç§’**ã€‚å®é™…ä¸Šï¼Œ1ms = 1,000,000nsï¼

So how is this "the same latency as DRAM"?

é‚£ä¹ˆè¿™æ€ä¹ˆèƒ½"å’Œ DRAM å»¶è¿Ÿç›¸å½“"å‘¢ï¼Ÿ

### The key insight: Sequential access

Here's where Carmack's genius shows: **AI doesn't need random access**.

è¿™å°±æ˜¯ Carmack çš„å¤©æ‰ä¹‹å¤„ï¼š**AI ä¸éœ€è¦éšæœºè®¿é—®**ã€‚

Traditional programs:
```python
# Random access pattern (éšæœºè®¿é—®æ¨¡å¼)
data[5]    # Jump to index 5
data[142]  # Jump to index 142
data[7]    # Jump back to index 7
```

AI inference:
```python
# Sequential access pattern (é¡ºåºè®¿é—®æ¨¡å¼)
for weight in model_weights:
    output += weight * input
```

AI reads model weights **in order**, one after another. It's like reading a book: you don't jump randomly between pages, you read page 1, then page 2, then page 3...

AI **æŒ‰é¡ºåº**è¯»å–æ¨¡å‹æƒé‡ï¼Œä¸€ä¸ªæ¥ä¸€ä¸ªã€‚å°±åƒè¯»ä¹¦ï¼šä½ ä¸ä¼šåœ¨é¡µé¢ä¹‹é—´éšæœºè·³è½¬ï¼Œä½ è¯»ç¬¬ 1 é¡µï¼Œç„¶åç¬¬ 2 é¡µï¼Œç„¶åç¬¬ 3 é¡µâ€¦â€¦

### How fiber solves this

If you know the data access pattern in advance, you can **pre-load** the fiber loop:

å¦‚æœä½ æå‰çŸ¥é“æ•°æ®è®¿é—®æ¨¡å¼ï¼Œä½ å¯ä»¥**é¢„åŠ è½½**å…‰çº¤ç¯ï¼š

```
Time 0ms:  Load data[0] into fiber
           (å°† data[0] åŠ è½½åˆ°å…‰çº¤)

Time 1ms:  data[0] arrives at detector, read it
           CPU requests data[1]
           (data[0] åˆ°è¾¾æ£€æµ‹å™¨ï¼Œè¯»å–å®ƒ
            CPU è¯·æ±‚ data[1])

Time 2ms:  data[1] arrives, read it
           CPU requests data[2]
           ...
```

**As long as you request data in order, the latency is effectively 0** â€” because the next piece of data is already on its way!

**åªè¦ä½ æŒ‰é¡ºåºè¯·æ±‚æ•°æ®ï¼Œå»¶è¿Ÿå®é™…ä¸Šå°±æ˜¯ 0** â€”â€” å› ä¸ºä¸‹ä¸€å—æ•°æ®å·²ç»åœ¨è·¯ä¸Šäº†ï¼

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“– è®¾è®¡èƒŒåçš„æ•…äº‹
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
**Delay-line memory** isn't new â€” it was used in early computers in the 1940s!

**å»¶è¿Ÿçº¿å­˜å‚¨å™¨**å¹¶ä¸æ–°é²œ â€”â€” å®ƒåœ¨ 1940 å¹´ä»£çš„æ—©æœŸè®¡ç®—æœºä¸­å°±è¢«ä½¿ç”¨äº†ï¼

Back then, they used **mercury tubes**: sound waves would travel through liquid mercury, storing data as acoustic pulses.

é‚£æ—¶å€™ï¼Œä»–ä»¬ä½¿ç”¨**æ°´é“¶ç®¡**ï¼šå£°æ³¢ä¼šç©¿è¿‡æ¶²æ€æ±ï¼Œå°†æ•°æ®å­˜å‚¨ä¸ºå£°è„‰å†²ã€‚

EDSAC (1949) used mercury delay lines to store 512 words of memory. Why? Because DRAM didn't exist yet!

EDSACï¼ˆ1949ï¼‰ä½¿ç”¨æ°´é“¶å»¶è¿Ÿçº¿å­˜å‚¨ 512 ä¸ªå­—çš„å†…å­˜ã€‚ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºé‚£æ—¶å€™ DRAM è¿˜ä¸å­˜åœ¨ï¼

Carmack's proposal is essentially: **"What if we bring back delay-line memory, but with lasers?"**

Carmack çš„ææ¡ˆæœ¬è´¨ä¸Šæ˜¯ï¼š**"å¦‚æœæˆ‘ä»¬é‡æ–°ä½¿ç”¨å»¶è¿Ÿçº¿å­˜å‚¨å™¨ï¼Œä½†ç”¨æ¿€å…‰å‘¢ï¼Ÿ"**
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

---

## Part 4: Why This Makes Sense for AI

### AI's unique access pattern

Traditional CPU workloads are unpredictable:
- Web browser: user clicks random links
- Video game: player moves in any direction
- Database: queries access random rows

ä¼ ç»Ÿ CPU å·¥ä½œè´Ÿè½½æ˜¯ä¸å¯é¢„æµ‹çš„ï¼š
- ç½‘é¡µæµè§ˆå™¨ï¼šç”¨æˆ·ç‚¹å‡»éšæœºé“¾æ¥
- ç”µå­æ¸¸æˆï¼šç©å®¶å‘ä»»ä½•æ–¹å‘ç§»åŠ¨
- æ•°æ®åº“ï¼šæŸ¥è¯¢è®¿é—®éšæœºè¡Œ

**AI inference is predictable:**
```
For each input:
    1. Load layer 1 weights (é¡ºåºè¯»å–)
    2. Compute layer 1 output
    3. Load layer 2 weights (é¡ºåºè¯»å–)
    4. Compute layer 2 output
    ...
```

**AI æ¨ç†æ˜¯å¯é¢„æµ‹çš„ï¼š**
- åŠ è½½ç¬¬ 1 å±‚æƒé‡ï¼ˆé¡ºåºè¯»å–ï¼‰
- è®¡ç®—ç¬¬ 1 å±‚è¾“å‡º
- åŠ è½½ç¬¬ 2 å±‚æƒé‡ï¼ˆé¡ºåºè¯»å–ï¼‰
- è®¡ç®—ç¬¬ 2 å±‚è¾“å‡º
- â€¦â€¦

### The power advantage

Light transmission requires almost **no power** to maintain â€” unlike DRAM, which must be constantly refreshed.

å…‰ä¼ è¾“å‡ ä¹**ä¸éœ€è¦åŠŸè€—**æ¥ç»´æŒ â€”â€” ä¸å¿…é¡»ä¸æ–­åˆ·æ–°çš„ DRAM ä¸åŒã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Power Comparison (åŠŸè€—æ¯”è¾ƒ)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  DRAM 16GB:  ~10W (idle)             â”‚
â”‚              ~15W (active)            â”‚
â”‚                                       â”‚
â”‚  Fiber 200km: ~1W (laser)            â”‚
â”‚               ~2W (amplifiers)        â”‚
â”‚                                       â”‚
â”‚  Savings: ~80%                        â”‚
â”‚  èŠ‚çœï¼š~80%                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

In a data center with thousands of servers, this could save **megawatts** of power.

åœ¨æ‹¥æœ‰æ•°åƒå°æœåŠ¡å™¨çš„æ•°æ®ä¸­å¿ƒä¸­ï¼Œè¿™å¯ä»¥èŠ‚çœ**æ•°å…†ç“¦**çš„åŠŸç‡ã€‚

### Bandwidth vs. Latency

Here's another insight: **AI cares more about bandwidth than latency**.

è¿™æ˜¯å¦ä¸€ä¸ªæ´å¯Ÿï¼š**AI æ›´å…³å¿ƒå¸¦å®½è€Œä¸æ˜¯å»¶è¿Ÿ**ã€‚

```
CPU mindset:  "I need this ONE piece of data NOW!"
              "æˆ‘ç°åœ¨éœ€è¦è¿™ä¸€å—æ•°æ®ï¼"
              â†’ Optimizes for low latency
              â†’ ä¼˜åŒ–ä½å»¶è¿Ÿ

AI mindset:   "I need this STREAM of data CONTINUOUSLY!"
              "æˆ‘éœ€è¦è¿™ä¸ªæ•°æ®æµæŒç»­ä¾›åº”ï¼"
              â†’ Optimizes for high bandwidth
              â†’ ä¼˜åŒ–é«˜å¸¦å®½
```

As long as the **data stream never stops**, the absolute latency doesn't matter.

åªè¦**æ•°æ®æµæ°¸ä¸åœæ­¢**ï¼Œç»å¯¹å»¶è¿Ÿå°±ä¸é‡è¦ã€‚

---

## Part 5: The Real Insight â€” Challenging Assumptions

### What Carmack is really saying

The 200km fiber proposal is a **thought experiment**, not a practical design. Carmack knows this. So what's his point?

200 å…¬é‡Œå…‰çº¤ææ¡ˆæ˜¯ä¸€ä¸ª**æ€ç»´å®éªŒ**ï¼Œè€Œä¸æ˜¯å®é™…è®¾è®¡ã€‚Carmack çŸ¥é“è¿™ä¸€ç‚¹ã€‚é‚£ä¹ˆä»–çš„é‡ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ

**He's challenging our assumptions about memory hierarchy.**

**ä»–åœ¨æŒ‘æˆ˜æˆ‘ä»¬å…³äºå†…å­˜å±‚æ¬¡ç»“æ„çš„å‡è®¾ã€‚**

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ å¸¸è§è¯¯åŒº
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âŒ **è¯¯åŒºï¼š** "DRAM is necessary because it's fast."
âŒ **è¯¯åŒºï¼š** "DRAM æ˜¯å¿…éœ€çš„ï¼Œå› ä¸ºå®ƒå¾ˆå¿«ã€‚"

âœ… **çœŸç›¸ï¼š** DRAM exists because CPUs need **random access**. If you don't need random access, you don't need DRAM.
âœ… **çœŸç›¸ï¼š** DRAM å­˜åœ¨æ˜¯å› ä¸º CPU éœ€è¦**éšæœºè®¿é—®**ã€‚å¦‚æœä½ ä¸éœ€è¦éšæœºè®¿é—®ï¼Œä½ å°±ä¸éœ€è¦ DRAMã€‚
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

### The practical proposal: Flash directly to AI chips

What Carmack actually suggests:
1. Take AI model weights (stored on flash/SSD)
2. Connect flash chips **directly** to AI accelerators
3. Skip DRAM entirely

Carmack å®é™…å»ºè®®çš„åšæ³•ï¼š
1. è·å– AI æ¨¡å‹æƒé‡ï¼ˆå­˜å‚¨åœ¨é—ªå­˜/SSD ä¸Šï¼‰
2. å°†é—ªå­˜èŠ¯ç‰‡**ç›´æ¥**è¿æ¥åˆ° AI åŠ é€Ÿå™¨
3. å®Œå…¨è·³è¿‡ DRAM

```
Traditional AI Server:
ä¼ ç»Ÿ AI æœåŠ¡å™¨ï¼š

â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”
â”‚ SSD â”‚ â”€â”€â†’ â”‚ DRAM â”‚ â”€â”€â†’ â”‚ GPU â”‚
â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜
            â†‘ Bottleneck! (ç“¶é¢ˆï¼)
            â†‘ Power hungry! (è€—ç”µï¼)

Carmack's Vision:
Carmack çš„æ„¿æ™¯ï¼š

â”Œâ”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”
â”‚Flashâ”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚ AI  â”‚
â”‚Arrayâ”‚   Direct connect â”‚Chip â”‚
â””â”€â”€â”€â”€â”€â”˜   (ç›´æ¥è¿æ¥)      â””â”€â”€â”€â”€â”€â”˜
          â†‘ No DRAM needed!
          â†‘ ä¸éœ€è¦ DRAMï¼
```

### Why this matters

The AI era is forcing us to rethink **everything**:
- Memory hierarchy (random vs. sequential)
- Power budgets (40% on DRAM is insane)
- Interface standards (PCIe? CXL? Direct?)

AI æ—¶ä»£æ­£è¿«ä½¿æˆ‘ä»¬é‡æ–°æ€è€ƒ**ä¸€åˆ‡**ï¼š
- å†…å­˜å±‚æ¬¡ç»“æ„ï¼ˆéšæœº vs. é¡ºåºï¼‰
- åŠŸè€—é¢„ç®—ï¼ˆ40% ç”¨äº DRAM æ˜¯ç–¯ç‹‚çš„ï¼‰
- æ¥å£æ ‡å‡†ï¼ˆPCIeï¼ŸCXLï¼Ÿç›´è¿ï¼Ÿï¼‰

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“– è®¾è®¡èƒŒåçš„æ•…äº‹
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
**This isn't the first time we've rethought memory.**

**è¿™ä¸æ˜¯æˆ‘ä»¬ç¬¬ä¸€æ¬¡é‡æ–°æ€è€ƒå†…å­˜ã€‚**

In the 1990s, GPUs introduced **unified memory** â€” CPU and GPU sharing the same RAM. Experts said it was crazy. Today, Apple's M-series chips do exactly this.

åœ¨ 1990 å¹´ä»£ï¼ŒGPU å¼•å…¥äº†**ç»Ÿä¸€å†…å­˜** â€”â€” CPU å’Œ GPU å…±äº«ç›¸åŒçš„ RAMã€‚ä¸“å®¶è¯´è¿™å¾ˆç–¯ç‹‚ã€‚ä»Šå¤©ï¼ŒApple çš„ M ç³»åˆ—èŠ¯ç‰‡æ­£æ˜¯è¿™æ ·åšçš„ã€‚

In the 2000s, Google built datacenters with **no local storage** â€” everything in RAM or network. Experts said it was wasteful. Today, it's standard for cloud computing.

åœ¨ 2000 å¹´ä»£ï¼ŒGoogle å»ºç«‹äº†**æ²¡æœ‰æœ¬åœ°å­˜å‚¨**çš„æ•°æ®ä¸­å¿ƒ â€”â€” ä¸€åˆ‡éƒ½åœ¨ RAM æˆ–ç½‘ç»œä¸­ã€‚ä¸“å®¶è¯´è¿™æ˜¯æµªè´¹ã€‚ä»Šå¤©ï¼Œè¿™æ˜¯äº‘è®¡ç®—çš„æ ‡å‡†ã€‚

**The pattern: When the workload changes, the architecture must change too.**

**è§„å¾‹ï¼šå½“å·¥ä½œè´Ÿè½½æ”¹å˜æ—¶ï¼Œæ¶æ„ä¹Ÿå¿…é¡»æ”¹å˜ã€‚**
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

---

## Summary: The Takeaway

Let's recap what we've learned:

è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹æˆ‘ä»¬å­¦åˆ°çš„ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Carmack's Fiber Cache Insight        â”‚
â”‚        (Carmack çš„å…‰çº¤ç¼“å­˜æ´å¯Ÿ)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  â¶ Traditional memory hierarchy assumes     â”‚
â”‚     random access (ä¼ ç»Ÿå†…å­˜å±‚æ¬¡å‡è®¾éšæœºè®¿é—®) â”‚
â”‚                                             â”‚
â”‚  â· AI workloads are sequential              â”‚
â”‚     (AI å·¥ä½œè´Ÿè½½æ˜¯é¡ºåºçš„)                   â”‚
â”‚                                             â”‚
â”‚  â¸ Sequential access doesn't need DRAM      â”‚
â”‚     (é¡ºåºè®¿é—®ä¸éœ€è¦ DRAM)                   â”‚
â”‚                                             â”‚
â”‚  â¹ Fiber is a thought experiment            â”‚
â”‚     (å…‰çº¤æ˜¯æ€ç»´å®éªŒ)                        â”‚
â”‚                                             â”‚
â”‚  âº Real solution: Flash â†’ AI chip directly  â”‚
â”‚     (çœŸæ­£çš„è§£å†³æ–¹æ¡ˆï¼šé—ªå­˜ â†’ AI èŠ¯ç‰‡ç›´è¿)    â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### One-sentence takeaway

**In the AI era, memory is no longer "fast random access storage" â€” it's "continuous data flow."**

**åœ¨ AI æ—¶ä»£ï¼Œå†…å­˜ä¸å†æ˜¯"å¿«é€Ÿéšæœºè®¿é—®å­˜å‚¨" â€”â€” è€Œæ˜¯"æŒç»­æ•°æ®æµ"ã€‚**

### Reflection question

> If you were designing an AI chip today, would you include DRAM support? Or would you design a completely new memory interface?
> 
> å¦‚æœä½ ä»Šå¤©è®¾è®¡ä¸€ä¸ª AI èŠ¯ç‰‡ï¼Œä½ ä¼šåŒ…æ‹¬ DRAM æ”¯æŒå—ï¼Ÿè¿˜æ˜¯ä½ ä¼šè®¾è®¡ä¸€ä¸ªå…¨æ–°çš„å†…å­˜æ¥å£ï¼Ÿ

### Next steps

Want to dive deeper? Explore these topics:
- **CXL (Compute Express Link)**: Industry standard for next-gen memory
- **HBM (High Bandwidth Memory)**: GPU memory architecture
- **Processing-in-Memory (PIM)**: Computing inside the memory chip
- **Optical Interconnects**: Using light for chip-to-chip communication

æƒ³æ·±å…¥äº†è§£ï¼Ÿæ¢ç´¢è¿™äº›ä¸»é¢˜ï¼š
- **CXLï¼ˆCompute Express Linkï¼‰**ï¼šä¸‹ä¸€ä»£å†…å­˜çš„è¡Œä¸šæ ‡å‡†
- **HBMï¼ˆé«˜å¸¦å®½å†…å­˜ï¼‰**ï¼šGPU å†…å­˜æ¶æ„
- **å†…å­˜å†…è®¡ç®—ï¼ˆPIMï¼‰**ï¼šåœ¨å†…å­˜èŠ¯ç‰‡å†…éƒ¨è®¡ç®—
- **å…‰äº’è¿**ï¼šä½¿ç”¨å…‰è¿›è¡ŒèŠ¯ç‰‡é—´é€šä¿¡

---

## ğŸ“ Final Thought

Carmack's 200km fiber proposal isn't about fiber at all â€” it's about **questioning defaults**.

Carmack çš„ 200 å…¬é‡Œå…‰çº¤ææ¡ˆæ ¹æœ¬ä¸æ˜¯å…³äºå…‰çº¤çš„ â€”â€” è€Œæ˜¯å…³äº**è´¨ç–‘é»˜è®¤å‡è®¾**ã€‚

When everyone is optimizing DRAM latency, he asks: **"Do we even need DRAM?"**

å½“æ‰€æœ‰äººéƒ½åœ¨ä¼˜åŒ– DRAM å»¶è¿Ÿæ—¶ï¼Œä»–é—®ï¼š**"æˆ‘ä»¬ç”šè‡³éœ€è¦ DRAM å—ï¼Ÿ"**

That's the mindset of a true systems thinker.

è¿™å°±æ˜¯çœŸæ­£çš„ç³»ç»Ÿæ€è€ƒè€…çš„å¿ƒæ€ã€‚

---

*Written in Japanese-style technical writing (æ—¥ç³»é£æ ¼æŠ€æœ¯å†™ä½œ)*
*Article length: ~3,500 words | Reading time: ~12 minutes*
*æ–‡ç« é•¿åº¦ï¼šçº¦ 3,500 å­— | é˜…è¯»æ—¶é—´ï¼šçº¦ 12 åˆ†é’Ÿ*
