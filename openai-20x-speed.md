# OpenAI突然发了个快20倍的模型，秘密武器竟然不是AI技术，而是一家要IPO的神秘公司

最近AI圈发生了一件大事。

OpenAI悄悄放出了一个新模型——GPT-5.3 Codex Spark。

这个模型有多快？

**1000 tokens/秒。**

什么概念？竞争对手最快的模型，大概也就50 tokens/秒。

也就是说，OpenAI这次的速度，是别人的**20倍**。

5秒钟，从一句话变成一个可以玩的贪吃蛇游戏。不是生成代码让你复制粘贴，是直接可以玩。

全网都炸了。

但今天我要告诉你的是：这件事最有意思的部分，跟AI模型技术**毫无关系**。

真正的秘密，藏在一家99%的人没听说过的公司里。

这家公司，今年很可能就要IPO了。

---

## 🚀 先说说这个模型到底有多离谱

GPT-5.3 Codex Spark，目前只对ChatGPT Pro用户开放（就是那个每月200美金的版本）。

它是OpenAI专门为**实时编程**设计的模型。

注意关键词：**实时**。

以前你用AI写代码，流程是这样的：你说需求→AI想个几秒十几秒→慢慢吐出代码→你复制过去试试→报错了→再问一次→再等十几秒……

现在用Codex Spark，流程变成了：你说需求→代码瞬间出来→你打断它说"不对，换个方向"→它立刻改→你再说"加个按钮"→立刻加。

就像跟一个反应极快的人结对编程。

你甚至可以在它写到一半的时候打断它，它不会卡住，不会重来，而是顺着你的新指令继续。

这不是"更快的AI"，这是一种**全新的人机交互方式**。

128K上下文窗口，纯文本模式，目前只在Codex应用、CLI和VS Code插件里可用。

在SWE-Bench Pro和Terminal-Bench 2.0两个业界标准测试中，它的编程能力跟GPT-5.3 Codex接近——但完成任务的时间只要后者的**一个零头**。

好，那问题来了：

**凭什么这么快？**

---

## 🤔 GPU不是万能的？

如果你问一个AI从业者："怎么让模型推理更快？"

十个人里有九个会告诉你：加GPU。

用更多的NVIDIA H100、B200，堆算力，做并行，上量化。整个AI行业过去三年就是这么干的。

但OpenAI这次选了一条完全不同的路。

他们找了一家公司，叫**Cerebras**。

你可能没听过这个名字。没关系，绝大多数人都没听过。

但我敢打赌，今年之后，你会反复看到它。

---

## 🧠 一块晶圆大的芯片：Cerebras到底是什么来头？

Cerebras做了一件所有半导体工程师都觉得疯狂的事：

他们把**整块硅晶圆**做成了**一颗芯片**。

你知道正常芯片多大吗？NVIDIA最强的B200，芯片面积大概800多平方毫米。

Cerebras的WSE-3（Wafer Scale Engine 3）有多大？

**46,255平方毫米。**

对，你没看错。大了将近**60倍**。

这颗芯片上面有：
- **4万亿个晶体管**（NVIDIA B200大概2080亿个，差了19倍）
- **90万个AI核心**
- **125 petaflops**的算力（是B200的28倍）

一块芯片，顶别人几十块。

为什么这很重要？因为AI推理的瓶颈不是算力不够，而是**数据搬运太慢**。

打个比方。

想象你是一个厨师。你灶台上火力充足（算力够），但问题是食材放在100米外的仓库里。每做一道菜，你都得跑到仓库拿食材，再跑回来。

你做菜慢，不是因为你炒菜不行，是因为你大部分时间在**跑腿**。

传统GPU集群就是这样。几十上百块GPU通过网线连在一起，数据在GPU之间来回传输，大量时间浪费在"搬运"上。

而Cerebras把所有东西——计算单元、内存、带宽——全塞进**一颗芯片**里。

食材就在灶台旁边。伸手就能拿到。

没有搬运延迟，没有通信瓶颈，没有网络堵车。

所以它能做到1000 tokens/秒这种离谱的速度。

---

## 💰 750兆瓦的合作：这笔交易有多大？

2026年1月，OpenAI和Cerebras正式宣布合作。

合作规模？**750兆瓦**的超低延迟AI算力。

750兆瓦是什么概念？一个中等城市的用电量大概就这个级别。

OpenAI的说法是："这是我们推理基础设施的补充，不是替代GPU，而是在GPU的基础上加一层超低延迟层。"

Sachin Katti（OpenAI基础设施负责人）说：

> "Cerebras为我们的平台增加了专用低延迟推理方案。这意味着更快的响应、更自然的交互、以及更强的基础来将实时AI扩展到更多人。"

Cerebras的CEO Andrew Feldman说了一句很有意思的话：

> "就像宽带改变了互联网一样，实时推理将改变AI。"

这句话值得细品。

想想2000年代初，我们从拨号上网切换到宽带。网速从56Kbps变成几Mbps。

表面上只是"快了一点"。但实际上？

YouTube诞生了。Netflix流媒体诞生了。在线游戏起飞了。整个互联网从"看文字"变成"看视频"。

不是量变，是**质变**。

1000 tokens/秒的AI，可能也是这样的质变时刻。

当AI的响应速度从"等几秒"变成"实时"，你跟它的关系就变了：从"我问它答"变成"我们一起干"。

---

## 🔧 不只是芯片：OpenAI自己也做了什么？

说公道话，Codex Spark的速度不全是Cerebras的功劳。

OpenAI在自己的推理栈上也做了大量优化。他们在官方博客里公布了几个数据：

- 客户端/服务器每次通信的开销，**降低了80%**
- 每个token的处理开销，**降低了30%**
- 首个token响应时间，**降低了50%**

怎么做到的？

一个关键改变：**引入了持久WebSocket连接**。

以前每次你跟AI对话，都是一个独立的HTTP请求。就像每次打电话都要先拨号、等接通、说完挂断、下次再拨。

现在用WebSocket，就像开了一条永远不挂断的热线电话。你随时说，它随时答，没有拨号等待的时间。

这个优化目前在Codex Spark上默认启用，后续会推广到所有模型。

所以完整的答案是：

**Cerebras的芯片负责"快"，OpenAI的工程优化负责"顺"。两者结合，才有了1000 tokens/秒的体验。**

---

## 📈 Cerebras要IPO了：为什么这件事值得关注？

Cerebras在2024年就提交了IPO申请（S-1文件），一度因为各种原因推迟。

但现在情况完全不同了。

他们拿到了OpenAI这个**超级客户**。750兆瓦的合同，产能一直到2028年才能完全交付。

这不是一笔小生意。这是一笔**改变公司命运**的合同。

想想看：

OpenAI现在是全球最大的AI应用公司。ChatGPT月活用户超过1亿。他们选择Cerebras作为推理基础设施的核心组件，这相当于什么？

相当于苹果在2007年选择了三星的OLED屏幕。

**一个顶级客户的背书，胜过一千份PPT。**

而且这不是一次性买卖。OpenAI说得很清楚："GPUs and Cerebras can be combined for single workloads to reach the best performance." GPU负责大规模推理，Cerebras负责超低延迟推理。两条腿走路。

这意味着Cerebras不是一个临时方案，而是OpenAI**长期推理架构**的一部分。

对投资者来说，这种确定性是极其稀缺的。

---

## 🌊 更大的图景：AI推理战争才刚开始

过去几年，所有人都在谈**训练**。谁的集群大，谁的GPU多，谁训出来的模型更聪明。

但2026年，风向变了。

模型越来越聪明，但用户体验的瓶颈不再是"模型不够好"，而是"模型太慢"。

OpenAI自己在博客里说了一句意味深长的话：

> "As models become more capable, interaction speed becomes a clear bottleneck."

当模型越来越强，交互速度就成了最明显的瓶颈。

这句话背后是一个巨大的产业逻辑：

**AI行业的下一个战场，不是训练，是推理。**

训练是一次性的。你花几个月、几亿美金训一个大模型，训完了就训完了。

但推理是**持续的**。每次用户跟ChatGPT说一句话，每次Codex帮你写一行代码，每次AI Agent自动执行一个任务——都是推理。

训练花的钱是一次性的，推理花的钱是**永恒的**。

而推理的核心指标就两个：快、便宜。

Cerebras选了"快"这条赛道，而且跑到了最前面。

---

## 🎯 对普通人意味着什么？

你可能会说："我又不炒股，Cerebras IPO不IPO跟我有什么关系？"

有关系。关系很大。

如果Cerebras成功了，如果这种超低延迟推理成为标配，那意味着：

**1. AI编程将从"辅助"变成"协作"**

现在你用GitHub Copilot，还是你写代码、它补全。以后可能是你说一句"做个电商后台"，然后你在旁边实时指挥AI写，就像指挥一个速度极快的实习生。

**2. AI Agent会真正起飞**

为什么现在AI Agent还不太行？一个很大的原因是慢。一个Agent执行一个任务，要调用模型十几次甚至几十次。每次推理都要等几秒，加起来就是几分钟。

如果推理速度快20倍？几分钟变成几秒钟。Agent才能真正做到"自动化"而不是"慢动作"。

**3. 新的应用场景会出现**

就像宽带催生了YouTube，1000 tokens/秒的AI可能催生出我们现在还想象不到的东西。

实时AI翻译同传？AI实时指导手术？AI实时生成游戏内容？

当速度不再是问题，想象力才是唯一的边界。

---

## 🏭 NVIDIA慌不慌？

这是很多人关心的问题。

答案是：**短期不慌，长期要警惕。**

OpenAI说得很明确：GPU仍然是训练和推理流水线的基础。Cerebras是"补充"不是"替代"。

但有意思的是，OpenAI用了一个词叫"resilient portfolio"（弹性组合）。这说明他们不想把所有鸡蛋放在NVIDIA一个篮子里。

对NVIDIA来说，真正的风险不是Cerebras今天能抢走多少市场份额，而是一个信号：

**AI大厂正在积极寻找GPU之外的方案。**

当苹果开始做自己的芯片，Intel的好日子就开始倒计时了。

当OpenAI开始用Cerebras做推理，NVIDIA也该想想自己的护城河到底有多深了。

---

## 📊 数据总结

| 指标 | 数据 |
|------|------|
| GPT-5.3 Codex Spark速度 | 1,000+ tokens/秒 |
| 对比竞争对手 | 约20倍速度优势 |
| 上下文窗口 | 128K |
| Cerebras WSE-3芯片面积 | 46,255 mm² |
| WSE-3晶体管数 | 4万亿 |
| WSE-3 AI核心数 | 90万 |
| WSE-3算力 | 125 petaflops |
| 对比NVIDIA B200 | 晶体管多19倍，算力多28倍 |
| OpenAI-Cerebras合作规模 | 750兆瓦 |
| 合作期限 | 至2028年 |
| 通信开销降低 | 80% |
| 每token开销降低 | 30% |
| 首token延迟降低 | 50% |
| 当前可用用户 | ChatGPT Pro用户 |

---

## 📖 参考来源

1. Ignacio de Gregorio, *"How Has OpenAI Released a 20x Faster Model?"*, Medium, 2026年2月
2. OpenAI官方博客, *"Introducing GPT-5.3-Codex-Spark"*, openai.com, 2026年2月
3. OpenAI官方博客, *"OpenAI partners with Cerebras"*, openai.com, 2026年1月
4. Cerebras官网, *"Wafer Scale Engine 3"*, cerebras.ai
5. Sean Lie (Cerebras CTO) 公开声明
6. Andrew Feldman (Cerebras CEO) 公开声明

---

*本文基于公开信息整理，不构成投资建议。数据截至2026年2月。*
